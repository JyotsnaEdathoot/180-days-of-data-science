{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "simplified-federation",
   "metadata": {},
   "source": [
    "# Emojify Deep Learning FER-2013\n",
    "\n",
    "Source : [Data-Flair](https://data-flair.training/blogs/create-emoji-with-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-cause",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "natural-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-release",
   "metadata": {},
   "source": [
    "## Initialize training and validation generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "julian-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/train'\n",
    "val_dir = 'data/test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                  train_dir,\n",
    "                  target_size=(48, 48),\n",
    "                  batch_size=64,\n",
    "                  color_mode='grayscale',\n",
    "                  class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "                       val_dir,\n",
    "                       target_size=(48, 48),\n",
    "                       batch_size=64,\n",
    "                       color_mode='grayscale',\n",
    "                       class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-tuner",
   "metadata": {},
   "source": [
    "## Build convolution neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reduced-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "################# LAYER 1 ###########################\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                         activation='relu', \n",
    "                         input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                         activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "################# LAYER 2 ###########################\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3),\n",
    "                         activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3),\n",
    "                         activation='relu'))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "################# LAYER 3 ###########################\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-memorabilia",
   "metadata": {},
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "portable-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 892s 2s/step - loss: 1.8083 - accuracy: 0.2610 - val_loss: 1.6141 - val_accuracy: 0.3811\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 227s 506ms/step - loss: 1.6047 - accuracy: 0.3816 - val_loss: 1.5106 - val_accuracy: 0.4224\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 250s 558ms/step - loss: 1.4996 - accuracy: 0.4273 - val_loss: 1.4279 - val_accuracy: 0.4555\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 249s 555ms/step - loss: 1.4246 - accuracy: 0.4578 - val_loss: 1.3722 - val_accuracy: 0.4750\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 256s 570ms/step - loss: 1.3442 - accuracy: 0.4914 - val_loss: 1.3238 - val_accuracy: 0.4939\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 240s 536ms/step - loss: 1.2949 - accuracy: 0.5083 - val_loss: 1.2942 - val_accuracy: 0.5049\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 247s 551ms/step - loss: 1.2372 - accuracy: 0.5349 - val_loss: 1.2482 - val_accuracy: 0.5240\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 216s 482ms/step - loss: 1.1735 - accuracy: 0.5604 - val_loss: 1.2245 - val_accuracy: 0.5338\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 228s 509ms/step - loss: 1.1223 - accuracy: 0.5807 - val_loss: 1.2137 - val_accuracy: 0.5363\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 211s 470ms/step - loss: 1.0640 - accuracy: 0.6060 - val_loss: 1.1757 - val_accuracy: 0.5576\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 200s 447ms/step - loss: 1.0117 - accuracy: 0.6304 - val_loss: 1.1708 - val_accuracy: 0.5632\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 200s 446ms/step - loss: 0.9666 - accuracy: 0.6421 - val_loss: 1.1636 - val_accuracy: 0.5660\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 184s 410ms/step - loss: 0.9030 - accuracy: 0.6734 - val_loss: 1.1601 - val_accuracy: 0.5720\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 210s 468ms/step - loss: 0.8445 - accuracy: 0.6963 - val_loss: 1.1772 - val_accuracy: 0.5732\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 239s 534ms/step - loss: 0.7956 - accuracy: 0.7142 - val_loss: 1.1714 - val_accuracy: 0.5760\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 199s 444ms/step - loss: 0.7206 - accuracy: 0.7406 - val_loss: 1.1664 - val_accuracy: 0.5852\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 197s 441ms/step - loss: 0.6784 - accuracy: 0.7555 - val_loss: 1.1776 - val_accuracy: 0.5843\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 196s 438ms/step - loss: 0.6264 - accuracy: 0.7734 - val_loss: 1.2111 - val_accuracy: 0.5943\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.5641 - accuracy: 0.8024 - val_loss: 1.2076 - val_accuracy: 0.5864\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.5290 - accuracy: 0.8110 - val_loss: 1.2540 - val_accuracy: 0.5901\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.4703 - accuracy: 0.8332 - val_loss: 1.2415 - val_accuracy: 0.5946\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 186s 414ms/step - loss: 0.4355 - accuracy: 0.8459 - val_loss: 1.3080 - val_accuracy: 0.5951\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 199s 444ms/step - loss: 0.4019 - accuracy: 0.8590 - val_loss: 1.3292 - val_accuracy: 0.5917\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 200s 447ms/step - loss: 0.3605 - accuracy: 0.8726 - val_loss: 1.3563 - val_accuracy: 0.5954\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 200s 445ms/step - loss: 0.3313 - accuracy: 0.8865 - val_loss: 1.3703 - val_accuracy: 0.5940\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 187s 417ms/step - loss: 0.3144 - accuracy: 0.8910 - val_loss: 1.4256 - val_accuracy: 0.5951\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.2892 - accuracy: 0.9025 - val_loss: 1.4291 - val_accuracy: 0.5933\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.2682 - accuracy: 0.9070 - val_loss: 1.4819 - val_accuracy: 0.5956\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 184s 410ms/step - loss: 0.2496 - accuracy: 0.9127 - val_loss: 1.4902 - val_accuracy: 0.5928\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 206s 461ms/step - loss: 0.2327 - accuracy: 0.9237 - val_loss: 1.5101 - val_accuracy: 0.5944\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 211s 471ms/step - loss: 0.2247 - accuracy: 0.9239 - val_loss: 1.5702 - val_accuracy: 0.5931\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 203s 453ms/step - loss: 0.2063 - accuracy: 0.9288 - val_loss: 1.5641 - val_accuracy: 0.5951\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 184s 410ms/step - loss: 0.1952 - accuracy: 0.9330 - val_loss: 1.5873 - val_accuracy: 0.5901\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 184s 410ms/step - loss: 0.1865 - accuracy: 0.9375 - val_loss: 1.6154 - val_accuracy: 0.6002\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 181s 404ms/step - loss: 0.1802 - accuracy: 0.9374 - val_loss: 1.6331 - val_accuracy: 0.5949\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 181s 405ms/step - loss: 0.1615 - accuracy: 0.9453 - val_loss: 1.6636 - val_accuracy: 0.5984\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 179s 400ms/step - loss: 0.1635 - accuracy: 0.9450 - val_loss: 1.7365 - val_accuracy: 0.5942\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 182s 405ms/step - loss: 0.1560 - accuracy: 0.9486 - val_loss: 1.6925 - val_accuracy: 0.5951\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 179s 400ms/step - loss: 0.1418 - accuracy: 0.9534 - val_loss: 1.7274 - val_accuracy: 0.5963\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 180s 401ms/step - loss: 0.1330 - accuracy: 0.9576 - val_loss: 1.7072 - val_accuracy: 0.5951\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 181s 404ms/step - loss: 0.1371 - accuracy: 0.9560 - val_loss: 1.8267 - val_accuracy: 0.5940\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 180s 402ms/step - loss: 0.1267 - accuracy: 0.9596 - val_loss: 1.7994 - val_accuracy: 0.5942\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 181s 404ms/step - loss: 0.1186 - accuracy: 0.9602 - val_loss: 1.7928 - val_accuracy: 0.6006\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 181s 403ms/step - loss: 0.1239 - accuracy: 0.9597 - val_loss: 1.8081 - val_accuracy: 0.5967\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 180s 401ms/step - loss: 0.1142 - accuracy: 0.9612 - val_loss: 1.8419 - val_accuracy: 0.5974\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 0.1135 - accuracy: 0.9618 - val_loss: 1.8181 - val_accuracy: 0.5968\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 281s 627ms/step - loss: 0.1082 - accuracy: 0.9631 - val_loss: 1.8383 - val_accuracy: 0.5974\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 257s 573ms/step - loss: 0.1005 - accuracy: 0.9666 - val_loss: 1.8131 - val_accuracy: 0.5926\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 250s 557ms/step - loss: 0.0999 - accuracy: 0.9683 - val_loss: 1.8145 - val_accuracy: 0.5887\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 265s 592ms/step - loss: 0.1044 - accuracy: 0.9655 - val_loss: 1.8830 - val_accuracy: 0.5887\n"
     ]
    }
   ],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=Adam(lr=0.0001, decay=1e-6), \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "emotion_model_info = emotion_model.fit(\n",
    "                     train_generator,\n",
    "                     steps_per_epoch=28709 // 64,\n",
    "                     epochs=50,\n",
    "                     validation_data=validation_generator,\n",
    "                     validation_steps=7178//64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-breath",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intensive-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "round-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-carol",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternate-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "emotion_model = load_model('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-sailing",
   "metadata": {},
   "source": [
    "## Integrating opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italic-northern",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-1i5nllza\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8f12d56c11fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mbounding_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mgray_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mnum_faces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbounding_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_frame\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnum_faces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-1i5nllza\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0 : \"Angry\",\n",
    "                1 : \"Disgusted\",\n",
    "                2 : \"Fearful\",\n",
    "                3 : \"Happy\",\n",
    "                4 : \"Neutral\",\n",
    "                5 : \"Sad\",\n",
    "                6 : \"Surprised\"}\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "  ret, frame = cap.read()\n",
    "  if not ret:\n",
    "    break\n",
    "  bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "  gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "  for (x, y, w, h) in num_faces:\n",
    "    cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "    roi_gray_frame = gray_frame[y:y+h, x:x+w]\n",
    "    cropped_image = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "    emotion_prediction = emotion_model.predict(cropped_img)\n",
    "    maxindex = int(np.argmax(emotion_prediction))\n",
    "    cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow(\"Video\", cv2.resize(frame, (1200, 860), interpolation = cv2.INTER_CUBIC))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "      break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
