{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solved-berry",
   "metadata": {},
   "source": [
    "# Deploying NLP Using Tensorflow JS\n",
    "\n",
    "Tutorial: [Dicoding](https://www.dicoding.com/academies)\n",
    "\n",
    "Dataset: [Sentiment Labelled Sentences Dataset](https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-vintage",
   "metadata": {},
   "source": [
    "## Import modules and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verbal-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "therapeutic-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "danish-claim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0                             Wow... Loved this place.      1\n",
       "1                                   Crust is not good.      0\n",
       "2            Not tasty and the texture was just nasty.      0\n",
       "3    Stopped by during the late May bank holiday of...      1\n",
       "4    The selection on the menu was great and so wer...      1\n",
       "..                                                 ...    ...\n",
       "995  I think food should have flavor and texture an...      0\n",
       "996                           Appetite instantly gone.      0\n",
       "997  Overall I was not impressed and would not go b...      0\n",
       "998  The whole experience was underwhelming, and I ...      0\n",
       "999  Then, as if I hadn't wasted enough of my life ...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-privilege",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "statutory-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all word become lowercase\n",
    "df['sentence'] = df['sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "affiliated-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing stopwords\n",
    "\n",
    "## Buat download file\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# But import file\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "comfortable-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengghilangkan stopwords (Kata yang umum digunakan)\n",
    "stop = set(stopwords.words('english'))\n",
    "df['sentence'] = df['sentence'].apply(lambda x:' ' \\\n",
    "                                      .join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "physical-george",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow... loved place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crust good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tasty texture nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selection menu great prices.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0                                wow... loved place.      1\n",
       "1                                        crust good.      0\n",
       "2                               tasty texture nasty.      0\n",
       "3  stopped late may bank holiday rick steve recom...      1\n",
       "4                       selection menu great prices.      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-label",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "moral-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "vocab_size = 2000\n",
    "oov_tok = \"<OOV>\"\n",
    "filters = '!\"#$%^&()*+.,-/:;=?@[\\]<>}{|_~`' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "diagnostic-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Pad Sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok, filters= filters)\n",
    "tokenizer.fit_on_texts(df['sentence'].values)\n",
    "\n",
    "word2index = tokenizer.word_index\n",
    "print(len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "prompt-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('word2index.json', 'w') as fp:\n",
    "    json.dump(word2index, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "grand-fifteen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(values.split()) for i, values in enumerate(df['sentence']))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "relevant-field",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 18)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_type='post'\n",
    "\n",
    "all_seq = tokenizer.texts_to_sequences(df['sentence'].values)\n",
    "all_padded = pad_sequences(all_seq, maxlen=max_length, padding=trunc_type)\n",
    "all_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-shannon",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "rapid-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "designed-activation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 18) (800,)\n",
      "(200, 18) (200,)\n"
     ]
    }
   ],
   "source": [
    "X = all_padded\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "protected-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
    "            tf.keras.layers.LSTM(64),\n",
    "#             tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "#             tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "demonstrated-period",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 18, 16)            32000     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                20736     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                1560      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 54,321\n",
      "Trainable params: 54,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aware-migration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 [==============================] - 3s 28ms/step - loss: 0.6933 - accuracy: 0.5075 - val_loss: 0.6938 - val_accuracy: 0.4800\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6870 - accuracy: 0.5800 - val_loss: 0.6637 - val_accuracy: 0.6100\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.3715 - accuracy: 0.8775 - val_loss: 0.6129 - val_accuracy: 0.7200\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.1585 - accuracy: 0.9450 - val_loss: 0.5763 - val_accuracy: 0.7700\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0652 - accuracy: 0.9837 - val_loss: 0.7397 - val_accuracy: 0.7750\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0388 - accuracy: 0.9875 - val_loss: 1.0086 - val_accuracy: 0.7550\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0335 - accuracy: 0.9862 - val_loss: 1.0567 - val_accuracy: 0.7650\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0227 - accuracy: 0.9950 - val_loss: 0.9470 - val_accuracy: 0.7600\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0140 - accuracy: 0.9962 - val_loss: 1.0933 - val_accuracy: 0.7700\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0091 - accuracy: 0.9987 - val_loss: 1.0619 - val_accuracy: 0.7700\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0092 - accuracy: 0.9987 - val_loss: 1.1314 - val_accuracy: 0.7650\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0087 - accuracy: 0.9987 - val_loss: 1.2338 - val_accuracy: 0.7550\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0130 - accuracy: 0.9975 - val_loss: 1.3336 - val_accuracy: 0.7600\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0092 - accuracy: 0.9987 - val_loss: 1.3061 - val_accuracy: 0.7700\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 1.3446 - val_accuracy: 0.7600\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0134 - accuracy: 0.9950 - val_loss: 1.2578 - val_accuracy: 0.7550\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0135 - accuracy: 0.9975 - val_loss: 1.5018 - val_accuracy: 0.7500\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0093 - accuracy: 0.9975 - val_loss: 1.3068 - val_accuracy: 0.7450\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 1.2401 - val_accuracy: 0.7500\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0102 - accuracy: 0.9987 - val_loss: 1.3691 - val_accuracy: 0.7700\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 1.5424 - val_accuracy: 0.7600\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0031 - accuracy: 0.9987 - val_loss: 1.5610 - val_accuracy: 0.7550\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0101 - accuracy: 0.9962 - val_loss: 1.7149 - val_accuracy: 0.7350\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0280 - accuracy: 0.9925 - val_loss: 0.9462 - val_accuracy: 0.7400\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0132 - accuracy: 0.9950 - val_loss: 1.2615 - val_accuracy: 0.7650\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 1.3897 - val_accuracy: 0.7750\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 8.2686e-04 - accuracy: 1.0000 - val_loss: 1.4976 - val_accuracy: 0.7550\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 5.3175e-04 - accuracy: 1.0000 - val_loss: 1.6356 - val_accuracy: 0.7550\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 3.4499e-04 - accuracy: 1.0000 - val_loss: 1.7013 - val_accuracy: 0.7600\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 1.6680e-04 - accuracy: 1.0000 - val_loss: 1.7386 - val_accuracy: 0.7550\n"
     ]
    }
   ],
   "source": [
    "epoch = 30\n",
    "history = model.fit(X_train, y_train, epochs=epoch, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-convention",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Convert text into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "demographic-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 18) for input KerasTensor(type_spec=TensorSpec(shape=(None, 18), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None, 5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 18) for input KerasTensor(type_spec=TensorSpec(shape=(None, 18), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None, 5).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.64647126]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_sequences(sentence):\n",
    "    pad = []\n",
    "    for stc in sentence.split():\n",
    "        if stc.lower() in word2index.keys():\n",
    "            pad.append(word2index[stc.lower()])\n",
    "        else:\n",
    "            continue\n",
    "    return pad\n",
    "\n",
    "text = to_sequences(\"nice idea and well priced\")\n",
    "text\n",
    "model.predict([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-reasoning",
   "metadata": {},
   "source": [
    "Those warnings above, displayed because I input only 5 sequence number and the model ask to fill with 18 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "improving-start",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [20, 1736, 254, 58, 413, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "billion-spare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99994195]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-software",
   "metadata": {},
   "source": [
    "The model said it labeled as almost 1 (Positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-airport",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aggressive-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflowjs # Syntax buat download tensorflow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "latest-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mymodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mymodel/assets\n"
     ]
    }
   ],
   "source": [
    "saved_path = 'mymodel/'\n",
    "tf.saved_model.save(model, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "laden-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing weight file tfjsmodel\\model.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-07 06:34:45.071900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\n",
      "2021-08-07 06:34:50.771577: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\n",
      "2021-08-07 06:34:50.799201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: GeForce MX150 computeCapability: 6.1\n",
      "coreClock: 1.5315GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\n",
      "2021-08-07 06:34:50.799227: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\n",
      "2021-08-07 06:34:50.810867: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\n",
      "2021-08-07 06:34:50.810897: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\n",
      "2021-08-07 06:34:50.816985: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\n",
      "2021-08-07 06:34:50.821033: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\n",
      "2021-08-07 06:34:50.825200: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2021-08-07 06:34:50.830015: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\n",
      "2021-08-07 06:34:50.834859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2021-08-07 06:34:50.834901: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-08-07 06:34:50.835333: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-07 06:34:50.836021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-07 06:34:50.836046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2021-08-07 06:34:54.842160: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-08-07 06:34:54.842398: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-08-07 06:34:54.844412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: GeForce MX150 computeCapability: 6.1\n",
      "coreClock: 1.5315GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s\n",
      "2021-08-07 06:34:54.844447: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-08-07 06:34:54.953424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-07 06:34:54.953445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-08-07 06:34:54.953452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-08-07 06:34:54.988782: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 249 nodes (238), 346 edges (335), time = 6.193ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.105ms.\n",
      "\n",
      "2021-08-07 06:34:55.319649: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.01ms.\n",
      "  model_pruner: Graph size after: 165 nodes (-40), 209 edges (-40), time = 2.288ms.\n",
      "  constant_folding: Graph size after: 158 nodes (-7), 202 edges (-7), time = 5.384ms.\n",
      "  arithmetic_optimizer: Graph size after: 158 nodes (0), 202 edges (0), time = 1.984ms.\n",
      "  dependency_optimizer: Graph size after: 142 nodes (-16), 162 edges (-40), time = 1.223ms.\n",
      "  model_pruner: Graph size after: 142 nodes (0), 162 edges (0), time = 0.607ms.\n",
      "  constant_folding: Graph size after: 142 nodes (0), 162 edges (0), time = 2.537ms.\n",
      "  arithmetic_optimizer: Graph size after: 142 nodes (0), 162 edges (0), time = 1.684ms.\n",
      "  dependency_optimizer: Graph size after: 142 nodes (0), 162 edges (0), time = 0.981ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.074ms.\n",
      "  model_pruner: Graph size after: 142 nodes (0), 162 edges (0), time = 0.541ms.\n",
      "  constant_folding: Graph size after: 142 nodes (0), 162 edges (0), time = 1.777ms.\n",
      "  arithmetic_optimizer: Graph size after: 142 nodes (0), 162 edges (0), time = 1.614ms.\n",
      "  dependency_optimizer: Graph size after: 142 nodes (0), 162 edges (0), time = 0.929ms.\n",
      "  model_pruner: Graph size after: 142 nodes (0), 162 edges (0), time = 0.601ms.\n",
      "  constant_folding: Graph size after: 142 nodes (0), 162 edges (0), time = 1.856ms.\n",
      "  arithmetic_optimizer: Graph size after: 142 nodes (0), 162 edges (0), time = 1.647ms.\n",
      "  dependency_optimizer: Graph size after: 142 nodes (0), 162 edges (0), time = 0.957ms.\n",
      "\n",
      "2021-08-07 06:34:55.417600: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  remapper: Graph size after: 139 nodes (-3), 159 edges (-3), time = 0.73ms.\n",
      "  constant_folding: Graph size after: 139 nodes (0), 159 edges (0), time = 2.595ms.\n",
      "  arithmetic_optimizer: Graph size after: 139 nodes (0), 159 edges (0), time = 1.63ms.\n",
      "  dependency_optimizer: Graph size after: 139 nodes (0), 159 edges (0), time = 0.982ms.\n",
      "  remapper: Graph size after: 139 nodes (0), 159 edges (0), time = 0.376ms.\n",
      "  constant_folding: Graph size after: 139 nodes (0), 159 edges (0), time = 1.735ms.\n",
      "  arithmetic_optimizer: Graph size after: 139 nodes (0), 159 edges (0), time = 1.56ms.\n",
      "  dependency_optimizer: Graph size after: 139 nodes (0), 159 edges (0), time = 0.928ms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the model, to make tfjs understand the model\n",
    "!tensorflowjs_converter \\\n",
    " --input_format=tf_saved_model \\\n",
    " mymodel/ \\\n",
    " tfjsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-angle",
   "metadata": {},
   "source": [
    "Model is ready for use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-responsibility",
   "metadata": {},
   "source": [
    "## Grab metadata\n",
    "\n",
    "#JustForNote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "joined-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['sentence'].values)\n",
    "\n",
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "piano-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"word2index.json\", \"w\") as fp:\n",
    "    json.dump(word2index, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
