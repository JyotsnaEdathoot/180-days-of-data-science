{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liberal-boost",
   "metadata": {},
   "source": [
    "# Main Process\n",
    "\n",
    "- Text Cleaning\n",
    "- Sentence Tokenization\n",
    "- Word Tokenization\n",
    "- Word-frequency table\n",
    "- Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-mixer",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "possible-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This article is about natural language processing done by computers. For the natural language processing done by the human brain, see Language processing in the brain. Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[19] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Latest works tend to use non-technical structure of a given task to build proper neural network.[20]\n",
    "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-millennium",
   "metadata": {},
   "source": [
    "## Install Important Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consistent-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.1.4-cp38-cp38-win_amd64.whl (12.0 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-win_amd64.whl (452 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy) (1.21.2)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-win_amd64.whl (113 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.2-py3-none-any.whl (40 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.0.2-py3-none-any.whl (133 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.12-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting pyparsing<3,>=2.0.2\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.0.1-cp38-cp38-win_amd64.whl (14 kB)\n",
      "Installing collected packages: colorama, murmurhash, cymem, click, catalogue, wasabi, typer, srsly, smart-open, pyparsing, pydantic, preshed, MarkupSafe, idna, charset-normalizer, blis, tqdm, thinc, spacy-legacy, requests, pathy, packaging, jinja2, spacy\n",
      "Successfully installed MarkupSafe-2.0.1 blis-0.7.5 catalogue-2.0.6 charset-normalizer-2.0.7 click-8.0.3 colorama-0.4.4 cymem-2.0.6 idna-3.3 jinja2-3.0.2 murmurhash-1.0.6 packaging-21.2 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 pyparsing-2.4.7 requests-2.26.0 smart-open-5.2.1 spacy-3.1.4 spacy-legacy-3.0.8 srsly-2.4.2 thinc-8.0.12 tqdm-4.62.3 typer-0.4.0 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "provincial-david",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from en-core-web-sm==3.1.0) (3.1.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.21.2)\n",
      "Requirement already satisfied: setuptools in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (58.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.5)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.3)\n",
      "Requirement already satisfied: colorama in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\asus\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-ready",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cloudy-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "declared-victory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beside',\n",
       " 'empty',\n",
       " 'about',\n",
       " 'must',\n",
       " 'further',\n",
       " 'is',\n",
       " 'from',\n",
       " 'thence',\n",
       " 'both',\n",
       " 'last',\n",
       " 'least',\n",
       " 'across',\n",
       " 'will',\n",
       " 'serious',\n",
       " 'and',\n",
       " 'would',\n",
       " 'us',\n",
       " 'no',\n",
       " 'were',\n",
       " 'we',\n",
       " 'fifty',\n",
       " 'that',\n",
       " 'less',\n",
       " 'keep',\n",
       " 'same',\n",
       " 'such',\n",
       " 'as',\n",
       " 'thru',\n",
       " 'else',\n",
       " 'everything',\n",
       " 'up',\n",
       " 'full',\n",
       " 'our',\n",
       " 'could',\n",
       " 'anyway',\n",
       " 'an',\n",
       " 'once',\n",
       " 'cannot',\n",
       " 'fifteen',\n",
       " 'five',\n",
       " 'should',\n",
       " 'top',\n",
       " 'whereupon',\n",
       " 'beforehand',\n",
       " 'someone',\n",
       " 'whereas',\n",
       " 'themselves',\n",
       " 'may',\n",
       " 'noone',\n",
       " 'regarding',\n",
       " 'after',\n",
       " '‘ve',\n",
       " 'two',\n",
       " 'each',\n",
       " 'several',\n",
       " 'therefore',\n",
       " 'whereby',\n",
       " 'sometime',\n",
       " 'twenty',\n",
       " 'what',\n",
       " 'only',\n",
       " 'among',\n",
       " 'other',\n",
       " 'under',\n",
       " 'of',\n",
       " 'again',\n",
       " 'together',\n",
       " 'between',\n",
       " 'none',\n",
       " 'most',\n",
       " 'anywhere',\n",
       " 'it',\n",
       " 'sixty',\n",
       " 'still',\n",
       " 'but',\n",
       " '‘m',\n",
       " 'done',\n",
       " 'do',\n",
       " 'there',\n",
       " 'much',\n",
       " 'by',\n",
       " 'just',\n",
       " 'nine',\n",
       " 'not',\n",
       " '’s',\n",
       " 'everyone',\n",
       " 'have',\n",
       " 'meanwhile',\n",
       " 'on',\n",
       " 'beyond',\n",
       " 'take',\n",
       " 'third',\n",
       " 'see',\n",
       " 'during',\n",
       " \"'s\",\n",
       " 'throughout',\n",
       " 'indeed',\n",
       " 'why',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'all',\n",
       " 'really',\n",
       " 'along',\n",
       " 'one',\n",
       " 'had',\n",
       " '’ve',\n",
       " 'those',\n",
       " 're',\n",
       " 'eight',\n",
       " 'rather',\n",
       " 'nor',\n",
       " 'own',\n",
       " 'she',\n",
       " 'which',\n",
       " 'thereafter',\n",
       " 'many',\n",
       " 'namely',\n",
       " 'name',\n",
       " 'a',\n",
       " 'has',\n",
       " 'being',\n",
       " 'himself',\n",
       " 'seems',\n",
       " 'wherein',\n",
       " 'seeming',\n",
       " 'above',\n",
       " 'whether',\n",
       " 'until',\n",
       " 'via',\n",
       " 'go',\n",
       " 'becoming',\n",
       " 'very',\n",
       " 'too',\n",
       " 'out',\n",
       " 'whither',\n",
       " 'i',\n",
       " 'or',\n",
       " 'anything',\n",
       " 'hereupon',\n",
       " 'them',\n",
       " 'below',\n",
       " 'formerly',\n",
       " 'n‘t',\n",
       " 'elsewhere',\n",
       " 'nobody',\n",
       " 'even',\n",
       " 'get',\n",
       " 'herein',\n",
       " 'him',\n",
       " 'ours',\n",
       " \"'ll\",\n",
       " 'seem',\n",
       " 'various',\n",
       " 'down',\n",
       " 'whole',\n",
       " 'otherwise',\n",
       " 'behind',\n",
       " 'whom',\n",
       " 'somehow',\n",
       " 'well',\n",
       " 'every',\n",
       " 'mostly',\n",
       " 'hereby',\n",
       " 'made',\n",
       " 'per',\n",
       " 'be',\n",
       " 'forty',\n",
       " 'something',\n",
       " 'nevertheless',\n",
       " 'upon',\n",
       " 'if',\n",
       " 'already',\n",
       " 'they',\n",
       " 'move',\n",
       " 'did',\n",
       " 'few',\n",
       " 'perhaps',\n",
       " 'around',\n",
       " 'yours',\n",
       " 'ca',\n",
       " 'doing',\n",
       " 'although',\n",
       " 'latterly',\n",
       " 'bottom',\n",
       " 'its',\n",
       " '’re',\n",
       " 'because',\n",
       " 'never',\n",
       " 'was',\n",
       " 'might',\n",
       " 'hundred',\n",
       " 'does',\n",
       " 'whose',\n",
       " 'some',\n",
       " 'first',\n",
       " 'nothing',\n",
       " 'thereupon',\n",
       " 'over',\n",
       " 'whereafter',\n",
       " 'the',\n",
       " 'neither',\n",
       " 'latter',\n",
       " 'either',\n",
       " 'put',\n",
       " 'how',\n",
       " 'quite',\n",
       " 'show',\n",
       " 'he',\n",
       " 'into',\n",
       " 'twelve',\n",
       " 'six',\n",
       " 'whoever',\n",
       " 'nowhere',\n",
       " 'am',\n",
       " 'yet',\n",
       " '’m',\n",
       " 'their',\n",
       " 'thus',\n",
       " '’d',\n",
       " 'back',\n",
       " 'can',\n",
       " 'seemed',\n",
       " 'through',\n",
       " 'besides',\n",
       " 'another',\n",
       " 'whatever',\n",
       " 'off',\n",
       " 'also',\n",
       " 'who',\n",
       " 'however',\n",
       " '’ll',\n",
       " 'my',\n",
       " 'hereafter',\n",
       " 'unless',\n",
       " 'amount',\n",
       " 'more',\n",
       " 'mine',\n",
       " 'than',\n",
       " 'toward',\n",
       " 'make',\n",
       " 'whenever',\n",
       " 'now',\n",
       " 'four',\n",
       " 'yourselves',\n",
       " 'onto',\n",
       " 'everywhere',\n",
       " 'amongst',\n",
       " 'though',\n",
       " 'then',\n",
       " 'ourselves',\n",
       " 'while',\n",
       " 'next',\n",
       " 'here',\n",
       " 'anyone',\n",
       " 'when',\n",
       " 'her',\n",
       " 'using',\n",
       " 'alone',\n",
       " '‘ll',\n",
       " 'where',\n",
       " 'without',\n",
       " 'so',\n",
       " 'been',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'part',\n",
       " 'ten',\n",
       " '‘re',\n",
       " 'three',\n",
       " \"'m\",\n",
       " 'become',\n",
       " '‘s',\n",
       " 'eleven',\n",
       " 'yourself',\n",
       " 'myself',\n",
       " 'others',\n",
       " 'at',\n",
       " 'to',\n",
       " 'with',\n",
       " 'against',\n",
       " 'n’t',\n",
       " 'herself',\n",
       " 'these',\n",
       " 'always',\n",
       " 'say',\n",
       " 'became',\n",
       " 'former',\n",
       " 'itself',\n",
       " 'please',\n",
       " 'afterwards',\n",
       " \"'re\",\n",
       " 'therein',\n",
       " 'used',\n",
       " 'any',\n",
       " \"'ve\",\n",
       " 'thereby',\n",
       " 'since',\n",
       " 'side',\n",
       " 'somewhere',\n",
       " 'call',\n",
       " 'hence',\n",
       " 'you',\n",
       " 'me',\n",
       " '‘d',\n",
       " 'whence',\n",
       " 'hers',\n",
       " 'except',\n",
       " 'are',\n",
       " 'wherever',\n",
       " 'your',\n",
       " 'due',\n",
       " 'this',\n",
       " 'in',\n",
       " 'ever',\n",
       " 'his',\n",
       " \"'d\",\n",
       " 'anyhow',\n",
       " 'sometimes',\n",
       " 'often',\n",
       " 'within',\n",
       " 'front',\n",
       " 'towards',\n",
       " 'give',\n",
       " \"n't\",\n",
       " 'moreover',\n",
       " 'becomes']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "french-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bright-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "basic-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tested-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'This', 'article', 'is', 'about', 'natural', 'language', 'processing', 'done', 'by', 'computers', '.', 'For', 'the', 'natural', 'language', 'processing', 'done', 'by', 'the', 'human', 'brain', ',', 'see', 'Language', 'processing', 'in', 'the', 'brain', '.', 'Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'goal', 'is', 'a', 'computer', 'capable', 'of', '\"', 'understanding', '\"', 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.', '\\n', 'A', 'major', 'drawback', 'of', 'statistical', 'methods', 'is', 'that', 'they', 'require', 'elaborate', 'feature', 'engineering', '.', 'Since', '2015,[19', ']', 'the', 'field', 'has', 'thus', 'largely', 'abandoned', 'statistical', 'methods', 'and', 'shifted', 'to', 'neural', 'networks', 'for', 'machine', 'learning', '.', 'Popular', 'techniques', 'include', 'the', 'use', 'of', 'word', 'embeddings', 'to', 'capture', 'semantic', 'properties', 'of', 'words', ',', 'and', 'an', 'increase', 'in', 'end', '-', 'to', '-', 'end', 'learning', 'of', 'a', 'higher', '-', 'level', 'task', '(', 'e.g.', ',', 'question', 'answering', ')', 'instead', 'of', 'relying', 'on', 'a', 'pipeline', 'of', 'separate', 'intermediate', 'tasks', '(', 'e.g.', ',', 'part', '-', 'of', '-', 'speech', 'tagging', 'and', 'dependency', 'parsing', ')', '.', 'In', 'some', 'areas', ',', 'this', 'shift', 'has', 'entailed', 'substantial', 'changes', 'in', 'how', 'NLP', 'systems', 'are', 'designed', ',', 'such', 'that', 'deep', 'neural', 'network', '-', 'based', 'approaches', 'may', 'be', 'viewed', 'as', 'a', 'new', 'paradigm', 'distinct', 'from', 'statistical', 'natural', 'language', 'processing', '.', 'For', 'instance', ',', 'the', 'term', 'neural', 'machine', 'translation', '(', 'NMT', ')', 'emphasizes', 'the', 'fact', 'that', 'deep', 'learning', '-', 'based', 'approaches', 'to', 'machine', 'translation', 'directly', 'learn', 'sequence', '-', 'to', '-', 'sequence', 'transformations', ',', 'obviating', 'the', 'need', 'for', 'intermediate', 'steps', 'such', 'as', 'word', 'alignment', 'and', 'language', 'modeling', 'that', 'was', 'used', 'in', 'statistical', 'machine', 'translation', '(', 'SMT', ')', '.', 'Latest', 'works', 'tend', 'to', 'use', 'non', '-', 'technical', 'structure', 'of', 'a', 'given', 'task', 'to', 'build', 'proper', 'neural', 'network.[20', ']', '\\n', 'Up', 'to', 'the', '1980s', ',', 'most', 'natural', 'language', 'processing', 'systems', 'were', 'based', 'on', 'complex', 'sets', 'of', 'hand', '-', 'written', 'rules', '.', 'Starting', 'in', 'the', 'late', '1980s', ',', 'however', ',', 'there', 'was', 'a', 'revolution', 'in', 'natural', 'language', 'processing', 'with', 'the', 'introduction', 'of', 'machine', 'learning', 'algorithms', 'for', 'language', 'processing', '.', 'This', 'was', 'due', 'to', 'both', 'the', 'steady', 'increase', 'in', 'computational', 'power', '(', 'see', 'Moore', \"'s\", 'law', ')', 'and', 'the', 'gradual', 'lessening', 'of', 'the', 'dominance', 'of', 'Chomskyan', 'theories', 'of', 'linguistics', '(', 'e.g.', 'transformational', 'grammar', ')', ',', 'whose', 'theoretical', 'underpinnings', 'discouraged', 'the', 'sort', 'of', 'corpus', 'linguistics', 'that', 'underlies', 'the', 'machine', '-', 'learning', 'approach', 'to', 'language', 'processing.[6', ']', '\\n']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "excess-slovak",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pleased-highland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = punctuation + '\\n'\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "palestinian-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "worth-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': 1, 'natural': 6, 'language': 12, 'processing': 8, 'computers': 3, 'human': 2, 'brain': 2, 'Language': 1, 'Natural': 1, 'NLP': 2, 'subfield': 1, 'linguistics': 3, 'computer': 2, 'science': 1, 'artificial': 1, 'intelligence': 1, 'concerned': 1, 'interactions': 1, 'particular': 1, 'program': 1, 'process': 1, 'analyze': 1, 'large': 1, 'amounts': 1, 'data': 1, 'goal': 1, 'capable': 1, 'understanding': 1, 'contents': 1, 'documents': 3, 'including': 1, 'contextual': 1, 'nuances': 1, 'technology': 1, 'accurately': 1, 'extract': 1, 'information': 1, 'insights': 1, 'contained': 1, 'categorize': 1, 'organize': 1, 'major': 1, 'drawback': 1, 'statistical': 4, 'methods': 2, 'require': 1, 'elaborate': 1, 'feature': 1, 'engineering': 1, '2015,[19': 1, 'field': 1, 'largely': 1, 'abandoned': 1, 'shifted': 1, 'neural': 4, 'networks': 1, 'machine': 6, 'learning': 5, 'Popular': 1, 'techniques': 1, 'include': 1, 'use': 2, 'word': 2, 'embeddings': 1, 'capture': 1, 'semantic': 1, 'properties': 1, 'words': 1, 'increase': 2, 'end': 2, 'higher': 1, 'level': 1, 'task': 2, 'e.g.': 3, 'question': 1, 'answering': 1, 'instead': 1, 'relying': 1, 'pipeline': 1, 'separate': 1, 'intermediate': 2, 'tasks': 1, 'speech': 1, 'tagging': 1, 'dependency': 1, 'parsing': 1, 'areas': 1, 'shift': 1, 'entailed': 1, 'substantial': 1, 'changes': 1, 'systems': 2, 'designed': 1, 'deep': 2, 'network': 1, 'based': 3, 'approaches': 2, 'viewed': 1, 'new': 1, 'paradigm': 1, 'distinct': 1, 'instance': 1, 'term': 1, 'translation': 3, 'NMT': 1, 'emphasizes': 1, 'fact': 1, 'directly': 1, 'learn': 1, 'sequence': 2, 'transformations': 1, 'obviating': 1, 'need': 1, 'steps': 1, 'alignment': 1, 'modeling': 1, 'SMT': 1, 'Latest': 1, 'works': 1, 'tend': 1, 'non': 1, 'technical': 1, 'structure': 1, 'given': 1, 'build': 1, 'proper': 1, 'network.[20': 1, '1980s': 2, 'complex': 1, 'sets': 1, 'hand': 1, 'written': 1, 'rules': 1, 'Starting': 1, 'late': 1, 'revolution': 1, 'introduction': 1, 'algorithms': 1, 'steady': 1, 'computational': 1, 'power': 1, 'Moore': 1, 'law': 1, 'gradual': 1, 'lessening': 1, 'dominance': 1, 'Chomskyan': 1, 'theories': 1, 'transformational': 1, 'grammar': 1, 'theoretical': 1, 'underpinnings': 1, 'discouraged': 1, 'sort': 1, 'corpus': 1, 'underlies': 1, 'approach': 1, 'processing.[6': 1}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "everyday-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = max(word_frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "resistant-reader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "wanted-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word] / max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "exposed-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': 0.08333333333333333, 'natural': 0.5, 'language': 1.0, 'processing': 0.6666666666666666, 'computers': 0.25, 'human': 0.16666666666666666, 'brain': 0.16666666666666666, 'Language': 0.08333333333333333, 'Natural': 0.08333333333333333, 'NLP': 0.16666666666666666, 'subfield': 0.08333333333333333, 'linguistics': 0.25, 'computer': 0.16666666666666666, 'science': 0.08333333333333333, 'artificial': 0.08333333333333333, 'intelligence': 0.08333333333333333, 'concerned': 0.08333333333333333, 'interactions': 0.08333333333333333, 'particular': 0.08333333333333333, 'program': 0.08333333333333333, 'process': 0.08333333333333333, 'analyze': 0.08333333333333333, 'large': 0.08333333333333333, 'amounts': 0.08333333333333333, 'data': 0.08333333333333333, 'goal': 0.08333333333333333, 'capable': 0.08333333333333333, 'understanding': 0.08333333333333333, 'contents': 0.08333333333333333, 'documents': 0.25, 'including': 0.08333333333333333, 'contextual': 0.08333333333333333, 'nuances': 0.08333333333333333, 'technology': 0.08333333333333333, 'accurately': 0.08333333333333333, 'extract': 0.08333333333333333, 'information': 0.08333333333333333, 'insights': 0.08333333333333333, 'contained': 0.08333333333333333, 'categorize': 0.08333333333333333, 'organize': 0.08333333333333333, 'major': 0.08333333333333333, 'drawback': 0.08333333333333333, 'statistical': 0.3333333333333333, 'methods': 0.16666666666666666, 'require': 0.08333333333333333, 'elaborate': 0.08333333333333333, 'feature': 0.08333333333333333, 'engineering': 0.08333333333333333, '2015,[19': 0.08333333333333333, 'field': 0.08333333333333333, 'largely': 0.08333333333333333, 'abandoned': 0.08333333333333333, 'shifted': 0.08333333333333333, 'neural': 0.3333333333333333, 'networks': 0.08333333333333333, 'machine': 0.5, 'learning': 0.4166666666666667, 'Popular': 0.08333333333333333, 'techniques': 0.08333333333333333, 'include': 0.08333333333333333, 'use': 0.16666666666666666, 'word': 0.16666666666666666, 'embeddings': 0.08333333333333333, 'capture': 0.08333333333333333, 'semantic': 0.08333333333333333, 'properties': 0.08333333333333333, 'words': 0.08333333333333333, 'increase': 0.16666666666666666, 'end': 0.16666666666666666, 'higher': 0.08333333333333333, 'level': 0.08333333333333333, 'task': 0.16666666666666666, 'e.g.': 0.25, 'question': 0.08333333333333333, 'answering': 0.08333333333333333, 'instead': 0.08333333333333333, 'relying': 0.08333333333333333, 'pipeline': 0.08333333333333333, 'separate': 0.08333333333333333, 'intermediate': 0.16666666666666666, 'tasks': 0.08333333333333333, 'speech': 0.08333333333333333, 'tagging': 0.08333333333333333, 'dependency': 0.08333333333333333, 'parsing': 0.08333333333333333, 'areas': 0.08333333333333333, 'shift': 0.08333333333333333, 'entailed': 0.08333333333333333, 'substantial': 0.08333333333333333, 'changes': 0.08333333333333333, 'systems': 0.16666666666666666, 'designed': 0.08333333333333333, 'deep': 0.16666666666666666, 'network': 0.08333333333333333, 'based': 0.25, 'approaches': 0.16666666666666666, 'viewed': 0.08333333333333333, 'new': 0.08333333333333333, 'paradigm': 0.08333333333333333, 'distinct': 0.08333333333333333, 'instance': 0.08333333333333333, 'term': 0.08333333333333333, 'translation': 0.25, 'NMT': 0.08333333333333333, 'emphasizes': 0.08333333333333333, 'fact': 0.08333333333333333, 'directly': 0.08333333333333333, 'learn': 0.08333333333333333, 'sequence': 0.16666666666666666, 'transformations': 0.08333333333333333, 'obviating': 0.08333333333333333, 'need': 0.08333333333333333, 'steps': 0.08333333333333333, 'alignment': 0.08333333333333333, 'modeling': 0.08333333333333333, 'SMT': 0.08333333333333333, 'Latest': 0.08333333333333333, 'works': 0.08333333333333333, 'tend': 0.08333333333333333, 'non': 0.08333333333333333, 'technical': 0.08333333333333333, 'structure': 0.08333333333333333, 'given': 0.08333333333333333, 'build': 0.08333333333333333, 'proper': 0.08333333333333333, 'network.[20': 0.08333333333333333, '1980s': 0.16666666666666666, 'complex': 0.08333333333333333, 'sets': 0.08333333333333333, 'hand': 0.08333333333333333, 'written': 0.08333333333333333, 'rules': 0.08333333333333333, 'Starting': 0.08333333333333333, 'late': 0.08333333333333333, 'revolution': 0.08333333333333333, 'introduction': 0.08333333333333333, 'algorithms': 0.08333333333333333, 'steady': 0.08333333333333333, 'computational': 0.08333333333333333, 'power': 0.08333333333333333, 'Moore': 0.08333333333333333, 'law': 0.08333333333333333, 'gradual': 0.08333333333333333, 'lessening': 0.08333333333333333, 'dominance': 0.08333333333333333, 'Chomskyan': 0.08333333333333333, 'theories': 0.08333333333333333, 'transformational': 0.08333333333333333, 'grammar': 0.08333333333333333, 'theoretical': 0.08333333333333333, 'underpinnings': 0.08333333333333333, 'discouraged': 0.08333333333333333, 'sort': 0.08333333333333333, 'corpus': 0.08333333333333333, 'underlies': 0.08333333333333333, 'approach': 0.08333333333333333, 'processing.[6': 0.08333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "likely-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", This article is about natural language processing done by computers., For the natural language processing done by the human brain, see Language processing in the brain., Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data., The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them., The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves., \n",
      ", A major drawback of statistical methods is that they require elaborate feature engineering., Since 2015,[19] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning., Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing)., In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing., For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT)., Latest works tend to use non-technical structure of a given task to build proper neural network.[20]\n",
      ", Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules., Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing., This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "suited-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "resistant-rebecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{This article is about natural language processing done by computers.: 2.5,\n",
       " For the natural language processing done by the human brain, see Language processing in the brain.: 4.333333333333333,\n",
       " Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.: 6.833333333333331,\n",
       " The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.: 2.0,\n",
       " The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.: 1.1666666666666667,\n",
       " A major drawback of statistical methods is that they require elaborate feature engineering.: 1.0,\n",
       " Since 2015,[19] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning.: 2.25,\n",
       " Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing).: 3.7500000000000018,\n",
       " In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.: 4.5,\n",
       " For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).: 6.583333333333333,\n",
       " Latest works tend to use non-technical structure of a given task to build proper neural network.[20]: 1.4166666666666665,\n",
       " Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.: 3.166666666666667,\n",
       " Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.: 5.250000000000001,\n",
       " This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]: 4.333333333333333}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "invalid-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "silver-tomato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_length = int(len(sentence_tokens) * 0.3)\n",
    "select_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "consecutive-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "patient-baking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.,\n",
       " For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).,\n",
       " Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.,\n",
       " In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "flexible-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = [word.text for word in summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "spiritual-cycling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
       " 'For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).',\n",
       " 'Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.',\n",
       " 'In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "mysterious-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ' '.join(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "meaning-wisdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "virtual-costs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "powerful-general",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This article is about natural language processing done by computers. For the natural language processing done by the human brain, see Language processing in the brain. Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[19] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Latest works tend to use non-technical structure of a given task to build proper neural network.[20]\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "protective-ribbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2544\n",
      "1007\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-interest",
   "metadata": {},
   "source": [
    "In this notebooks I success summarize an article from [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing). Actually I follow video from KGP Talkie title `NLP Tutorial 12 - Text Summarization using NLP`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
